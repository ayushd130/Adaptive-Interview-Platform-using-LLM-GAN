{"file_contents":{"app.py":{"content":"import os\nimport logging\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_login import LoginManager\nfrom sqlalchemy.orm import DeclarativeBase\nfrom werkzeug.middleware.proxy_fix import ProxyFix\n\n# Configure logging\nlogging.basicConfig(level=logging.DEBUG)\n\nclass Base(DeclarativeBase):\n    pass\n\ndb = SQLAlchemy(model_class=Base)\n\n# Create the app\napp = Flask(__name__)\napp.secret_key = os.environ.get(\"SESSION_SECRET\")\napp.wsgi_app = ProxyFix(app.wsgi_app, x_proto=1, x_host=1)\n\n# Configure the database\napp.config[\"SQLALCHEMY_DATABASE_URI\"] = os.environ.get(\"DATABASE_URL\")\napp.config[\"SQLALCHEMY_ENGINE_OPTIONS\"] = {\n    \"pool_recycle\": 300,\n    \"pool_pre_ping\": True,\n}\n\n# Initialize the app with extensions\ndb.init_app(app)\n\n# Initialize Flask-Login\nlogin_manager = LoginManager()\nlogin_manager.init_app(app)\nlogin_manager.login_view = 'login'  # type: ignore\nlogin_manager.login_message = 'Please log in to access this page.'\n\n@login_manager.user_loader\ndef load_user(user_id):\n    from models import User\n    return User.query.get(int(user_id))\n\nwith app.app_context():\n    # Import models to ensure tables are created\n    import models\n    db.create_all()\n    logging.info(\"Database tables created successfully\")\n","size_bytes":1247},"gan_service.py":{"content":"import numpy as np  # type: ignore\nimport random\nimport logging\nfrom typing import List, Dict, Any\n\nclass MockGANService:\n    \"\"\"\n    Mock GAN service for question diversity enhancement\n    In a production environment, this would be replaced with an actual GAN model\n    \"\"\"\n    \n    def __init__(self):\n        self.question_templates = {\n            'technical': {\n                'programming': [\n                    \"Implement a {algorithm} in {language} and explain its time complexity.\",\n                    \"How would you optimize a {problem_type} problem in {language}?\",\n                    \"Design a {data_structure} that supports {operations}.\",\n                    \"Explain the trade-offs between {concept1} and {concept2} in {context}.\"\n                ],\n                'system_design': [\n                    \"Design a {system_type} that can handle {scale} users.\",\n                    \"How would you implement {feature} in a distributed {system}?\",\n                    \"What are the challenges of building a {service_type} and how would you address them?\",\n                    \"Explain how you would scale a {application_type} from {start_scale} to {end_scale}.\"\n                ],\n                'algorithms': [\n                    \"Solve the {problem_name} problem using {approach}.\",\n                    \"Given {input_type}, find the {output_type} in optimal time.\",\n                    \"Implement {algorithm_type} for {use_case}.\",\n                    \"Optimize this {problem_category} problem for {constraint}.\"\n                ]\n            },\n            'non-technical': {\n                'behavioral': [\n                    \"Tell me about a time when you {situation} and how you handled it.\",\n                    \"Describe a situation where you had to {challenge} and what was the outcome.\",\n                    \"Give me an example of when you {action} and what you learned from it.\",\n                    \"How did you handle a time when you {conflict_situation}?\"\n                ],\n                'situational': [\n                    \"If you were {scenario}, how would you approach it?\",\n                    \"What would you do if {challenging_situation} occurred?\",\n                    \"How would you handle {workplace_scenario}?\",\n                    \"If faced with {decision_scenario}, what factors would you consider?\"\n                ],\n                'leadership': [\n                    \"Describe your experience with {leadership_activity}.\",\n                    \"How do you {leadership_skill} in a team environment?\",\n                    \"What's your approach to {management_challenge}?\",\n                    \"How would you {leadership_scenario} with your team?\"\n                ]\n            }\n        }\n        \n        self.variable_pools = {\n            'algorithm': ['binary search', 'quicksort', 'merge sort', 'dijkstra', 'BFS', 'DFS'],\n            'language': ['Python', 'Java', 'JavaScript', 'C++', 'Go'],\n            'problem_type': ['search', 'sorting', 'graph traversal', 'dynamic programming'],\n            'data_structure': ['hash table', 'binary tree', 'graph', 'trie', 'heap'],\n            'operations': ['insertion, deletion, and search', 'range queries', 'updates and queries'],\n            'concept1': ['arrays', 'linked lists', 'recursion', 'iteration'],\n            'concept2': ['hash tables', 'trees', 'dynamic programming', 'greedy algorithms'],\n            'context': ['memory usage', 'performance', 'scalability', 'maintainability'],\n            'system_type': ['chat application', 'video streaming service', 'e-commerce platform', 'social media platform'],\n            'scale': ['1 million', '10 million', '100 million', '1 billion'],\n            'feature': ['real-time notifications', 'user authentication', 'data analytics', 'content recommendation'],\n            'system': ['system', 'microservices architecture', 'cloud environment'],\n            'service_type': ['payment processing system', 'search engine', 'recommendation engine'],\n            'application_type': ['web application', 'mobile app', 'API service'],\n            'start_scale': ['1000 users', '10K users', '100K users'],\n            'end_scale': ['1M users', '10M users', '100M users'],\n            'situation': ['faced a difficult deadline', 'disagreed with your manager', 'had to learn a new technology quickly'],\n            'challenge': ['work with a difficult team member', 'resolve a complex problem', 'adapt to sudden changes'],\n            'action': ['took initiative on a project', 'mentored a colleague', 'improved a process'],\n            'conflict_situation': ['your idea was rejected', 'you made a mistake', 'priorities changed suddenly'],\n            'scenario': ['leading a project with tight deadlines', 'managing conflicting priorities', 'working with limited resources'],\n            'challenging_situation': ['a team member was underperforming', 'you disagreed with a major decision', 'a project was failing'],\n            'workplace_scenario': ['communication breakdown in your team', 'resistance to change', 'conflicting stakeholder requirements'],\n            'decision_scenario': ['choosing between two equally important projects', 'allocating limited budget', 'hiring decisions'],\n            'leadership_activity': ['leading a cross-functional team', 'managing remote workers', 'driving organizational change'],\n            'leadership_skill': ['motivate team members', 'delegate effectively', 'provide feedback'],\n            'management_challenge': ['performance management', 'conflict resolution', 'strategic planning'],\n            'leadership_scenario': ['build consensus', 'manage underperformance', 'drive innovation']\n        }\n    \n    def generate_diverse_questions(self, base_questions: List[Dict[str, Any]], diversity_factor: float = 0.3) -> List[Dict[str, Any]]:\n        \"\"\"\n        Generate diverse variations of base questions using GAN-like approach\n        diversity_factor: 0.0 (no diversity) to 1.0 (maximum diversity)\n        \"\"\"\n        try:\n            diverse_questions = []\n            \n            for question_data in base_questions:\n                # Apply GAN-like transformations based on diversity factor\n                if random.random() < diversity_factor:\n                    # Generate a completely new question using templates\n                    new_question = self._generate_from_template(\n                        question_data['type'], \n                        question_data.get('category', 'general'),\n                        question_data['difficulty']\n                    )\n                    if new_question:\n                        diverse_questions.append(new_question)\n                    else:\n                        diverse_questions.append(question_data)\n                else:\n                    # Apply minor variations to existing question\n                    varied_question = self._apply_variation(question_data)\n                    diverse_questions.append(varied_question)\n            \n            return diverse_questions\n            \n        except Exception as e:\n            logging.error(f\"Error in GAN service: {e}\")\n            return base_questions\n    \n    def _generate_from_template(self, question_type: str, category: str, difficulty: str) -> Dict[str, Any]:\n        \"\"\"Generate a new question from templates\"\"\"\n        try:\n            if question_type not in self.question_templates:\n                return {}\n            \n            if category not in self.question_templates[question_type]:\n                category = random.choice(list(self.question_templates[question_type].keys()))\n            \n            template = random.choice(self.question_templates[question_type][category])\n            \n            # Fill in template variables\n            question_text = self._fill_template(template)\n            \n            return {\n                'question': question_text,\n                'type': question_type,\n                'difficulty': difficulty,\n                'category': category,\n                'generated_by_gan': True\n            }\n            \n        except Exception as e:\n            logging.error(f\"Error generating from template: {e}\")\n            return {}\n    \n    def _fill_template(self, template: str) -> str:\n        \"\"\"Fill template with random variables\"\"\"\n        import re\n        \n        # Find all variables in template (format: {variable_name})\n        variables = re.findall(r'\\{(\\w+)\\}', template)\n        \n        filled_template = template\n        for var in variables:\n            if var in self.variable_pools:\n                replacement = random.choice(self.variable_pools[var])\n                filled_template = filled_template.replace(f'{{{var}}}', replacement)\n        \n        return filled_template\n    \n    def _apply_variation(self, question_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Apply minor variations to existing questions\"\"\"\n        question_text = question_data['question']\n        \n        # Simple variations: add context, change phrasing slightly\n        variations = [\n            f\"Can you {question_text.lower()}\",\n            f\"Please explain how you would {question_text.lower()}\",\n            f\"In your experience, {question_text.lower()}\",\n            f\"From a practical standpoint, {question_text.lower()}\"\n        ]\n        \n        # 30% chance to apply variation\n        if random.random() < 0.3:\n            question_data = question_data.copy()\n            question_data['question'] = random.choice(variations)\n            question_data['varied_by_gan'] = True\n        \n        return question_data\n    \n    def calculate_diversity_score(self, questions: List[Dict[str, Any]]) -> float:\n        \"\"\"Calculate how diverse the question set is\"\"\"\n        if not questions:\n            return 0.0\n        \n        # Simple diversity calculation based on:\n        # 1. Category distribution\n        # 2. Question length variation\n        # 3. Keyword diversity\n        \n        categories = [q.get('category', 'unknown') for q in questions]\n        category_diversity = len(set(categories)) / len(categories) if categories else 0\n        \n        lengths = [len(q['question'].split()) for q in questions]\n        length_variance = np.var(lengths) / np.mean(lengths) if lengths else 0\n        length_diversity = min(length_variance, 1.0)  # Cap at 1.0\n        \n        # Overall diversity score\n        diversity_score = (category_diversity + length_diversity) / 2\n        \n        return min(diversity_score, 1.0)\n    \n    def enhance_question_quality(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Use GAN-like techniques to enhance question quality\"\"\"\n        enhanced_questions = []\n        \n        for question_data in questions:\n            enhanced = question_data.copy()\n            \n            # Add complexity scoring\n            complexity_score = self._calculate_complexity(question_data['question'])\n            enhanced['complexity_score'] = complexity_score\n            \n            # Add semantic tags\n            semantic_tags = self._extract_semantic_tags(question_data['question'], question_data['type'])\n            enhanced['semantic_tags'] = semantic_tags\n            \n            enhanced_questions.append(enhanced)\n        \n        return enhanced_questions\n    \n    def _calculate_complexity(self, question: str) -> float:\n        \"\"\"Calculate question complexity score\"\"\"\n        # Simple complexity metrics\n        word_count = len(question.split())\n        unique_words = len(set(question.lower().split()))\n        avg_word_length = np.mean([len(word) for word in question.split()])\n        \n        # Normalize and combine metrics\n        complexity = (\n            min(word_count / 50, 1.0) * 0.4 +  # Word count factor\n            min(unique_words / word_count, 1.0) * 0.3 +  # Vocabulary diversity\n            min(avg_word_length / 10, 1.0) * 0.3  # Word complexity\n        )\n        \n        return complexity\n    \n    def _extract_semantic_tags(self, question: str, question_type: str) -> List[str]:\n        \"\"\"Extract semantic tags from question\"\"\"\n        tags = []\n        \n        # Technical keywords\n        technical_keywords = {\n            'algorithm', 'data structure', 'complexity', 'optimization', 'database',\n            'system design', 'architecture', 'scaling', 'performance', 'security'\n        }\n        \n        # Behavioral keywords\n        behavioral_keywords = {\n            'experience', 'challenge', 'conflict', 'leadership', 'teamwork',\n            'problem solving', 'communication', 'time management', 'decision making'\n        }\n        \n        question_lower = question.lower()\n        \n        if question_type == 'technical':\n            for keyword in technical_keywords:\n                if keyword in question_lower:\n                    tags.append(keyword)\n        else:\n            for keyword in behavioral_keywords:\n                if keyword in question_lower:\n                    tags.append(keyword)\n        \n        return tags\n","size_bytes":13042},"gemini_service.py":{"content":"import os\nimport json\nimport logging\nfrom google import genai\nfrom google.genai import types\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any\n\n# Initialize Gemini client\nclient = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\", \"default_key\"))\n\nclass QuestionResponse(BaseModel):\n    questions: List[Dict[str, str]]\n\nclass AnalysisResponse(BaseModel):\n    technical_accuracy: float\n    communication_score: float\n    confidence_score: float\n    feedback: str\n    improvement_suggestions: str\n\nclass InterviewAnalysisResponse(BaseModel):\n    overall_performance: float\n    technical_skills_score: float\n    communication_skills_score: float\n    confidence_level: float\n    strengths: str\n    areas_for_improvement: str\n    detailed_feedback: str\n\ndef generate_interview_questions(interview_type: str, difficulty: str = \"medium\", count: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"Generate interview questions using Gemini API\"\"\"\n    try:\n        if interview_type == \"technical\":\n            system_prompt = (\n                f\"You are an expert technical interviewer. Generate {count} unique {difficulty} level \"\n                \"technical interview questions covering programming, algorithms, system design, and problem-solving. \"\n                \"Each question should be distinct and test different aspects of technical knowledge. \"\n                \"Respond with JSON in this format: \"\n                \"{'questions': [{'question': 'question text', 'type': 'technical', 'difficulty': 'level', 'category': 'programming/algorithms/system_design'}]}\"\n            )\n        else:\n            system_prompt = (\n                f\"You are an expert HR interviewer. Generate {count} unique {difficulty} level \"\n                \"non-technical interview questions covering behavioral, situational, and soft skills assessment. \"\n                \"Each question should be distinct and test different aspects of professional competency. \"\n                \"Respond with JSON in this format: \"\n                \"{'questions': [{'question': 'question text', 'type': 'non-technical', 'difficulty': 'level', 'category': 'behavioral/situational/leadership'}]}\"\n            )\n\n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=[types.Content(role=\"user\", parts=[types.Part(text=system_prompt)])],\n            config=types.GenerateContentConfig(\n                response_mime_type=\"application/json\",\n                response_schema=QuestionResponse,\n            ),\n        )\n\n        if response.text:\n            data = json.loads(response.text)\n            return data.get('questions', [])\n        else:\n            raise ValueError(\"Empty response from Gemini API\")\n\n    except Exception as e:\n        logging.error(f\"Error generating questions: {e}\")\n        return generate_fallback_questions(interview_type, difficulty, count)\n\ndef analyze_user_response(question: str, user_answer: str, question_type: str) -> Dict[str, Any]:\n    \"\"\"Analyze user's response using Gemini API\"\"\"\n    try:\n        system_prompt = (\n            f\"You are an expert interview assessor. Analyze this interview response:\\n\\n\"\n            f\"Question: {question}\\n\"\n            f\"Answer: {user_answer}\\n\"\n            f\"Question Type: {question_type}\\n\\n\"\n            \"Provide scores (0-10) for technical accuracy, communication skills, and confidence. \"\n            \"Also provide detailed feedback and specific improvement suggestions. \"\n            \"Respond with JSON in this format: \"\n            \"{'technical_accuracy': float, 'communication_score': float, 'confidence_score': float, \"\n            \"'feedback': 'detailed feedback', 'improvement_suggestions': 'specific suggestions'}\"\n        )\n\n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=[types.Content(role=\"user\", parts=[types.Part(text=system_prompt)])],\n            config=types.GenerateContentConfig(\n                response_mime_type=\"application/json\",\n                response_schema=AnalysisResponse,\n            ),\n        )\n\n        if response.text:\n            data = json.loads(response.text)\n            return {\n                'technical_accuracy': data.get('technical_accuracy', 5.0) / 10.0,  # Convert to 0-1 scale\n                'communication_score': data.get('communication_score', 5.0) / 10.0,\n                'confidence_score': data.get('confidence_score', 5.0) / 10.0,\n                'feedback': data.get('feedback', 'No feedback available'),\n                'improvement_suggestions': data.get('improvement_suggestions', 'No suggestions available')\n            }\n        else:\n            raise ValueError(\"Empty response from Gemini API\")\n\n    except Exception as e:\n        logging.error(f\"Error analyzing response: {e}\")\n        return {\n            'technical_accuracy': 0.5,\n            'communication_score': 0.5,\n            'confidence_score': 0.5,\n            'feedback': 'Analysis temporarily unavailable',\n            'improvement_suggestions': 'Please try again later'\n        }\n\ndef generate_interview_analytics(questions_data: List[Dict], face_analysis_data: List[Dict]) -> Dict[str, Any]:\n    \"\"\"Generate comprehensive interview analytics using Gemini API\"\"\"\n    try:\n        analytics_prompt = (\n            \"You are an expert interview analyst. Based on the following interview data, \"\n            \"provide comprehensive analytics and insights:\\n\\n\"\n            f\"Questions and Responses: {json.dumps(questions_data, indent=2)}\\n\"\n            f\"Face Analysis Data: {json.dumps(face_analysis_data, indent=2)}\\n\\n\"\n            \"Analyze overall performance, technical skills, communication skills, confidence level. \"\n            \"Identify strengths, areas for improvement, and provide detailed feedback. \"\n            \"Scores should be between 0-10. \"\n            \"Respond with JSON in this format: \"\n            \"{'overall_performance': float, 'technical_skills_score': float, 'communication_skills_score': float, \"\n            \"'confidence_level': float, 'strengths': 'text', 'areas_for_improvement': 'text', 'detailed_feedback': 'text'}\"\n        )\n\n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=[types.Content(role=\"user\", parts=[types.Part(text=analytics_prompt)])],\n            config=types.GenerateContentConfig(\n                response_mime_type=\"application/json\",\n                response_schema=InterviewAnalysisResponse,\n            ),\n        )\n\n        if response.text:\n            data = json.loads(response.text)\n            return {\n                'overall_performance': data.get('overall_performance', 5.0) / 10.0,\n                'technical_skills_score': data.get('technical_skills_score', 5.0) / 10.0,\n                'communication_skills_score': data.get('communication_skills_score', 5.0) / 10.0,\n                'confidence_level': data.get('confidence_level', 5.0) / 10.0,\n                'strengths': data.get('strengths', 'Analysis pending'),\n                'areas_for_improvement': data.get('areas_for_improvement', 'Analysis pending'),\n                'detailed_feedback': data.get('detailed_feedback', 'Analysis pending')\n            }\n        else:\n            raise ValueError(\"Empty response from Gemini API\")\n\n    except Exception as e:\n        logging.error(f\"Error generating analytics: {e}\")\n        return {\n            'overall_performance': 0.5,\n            'technical_skills_score': 0.5,\n            'communication_skills_score': 0.5,\n            'confidence_level': 0.5,\n            'strengths': 'Analytics temporarily unavailable',\n            'areas_for_improvement': 'Please try again later',\n            'detailed_feedback': 'Detailed analysis will be available shortly'\n        }\n\ndef generate_fallback_questions(interview_type: str, difficulty: str, count: int) -> List[Dict[str, Any]]:\n    \"\"\"Fallback questions when Gemini API is unavailable\"\"\"\n    if interview_type == \"technical\":\n        return [\n            {\n                \"question\": \"Explain the difference between stack and queue data structures.\",\n                \"type\": \"technical\",\n                \"difficulty\": difficulty,\n                \"category\": \"algorithms\"\n            },\n            {\n                \"question\": \"What is the time complexity of binary search and why?\",\n                \"type\": \"technical\", \n                \"difficulty\": difficulty,\n                \"category\": \"algorithms\"\n            },\n            {\n                \"question\": \"Describe the principles of object-oriented programming.\",\n                \"type\": \"technical\",\n                \"difficulty\": difficulty,\n                \"category\": \"programming\"\n            },\n            {\n                \"question\": \"How would you design a URL shortening service like bit.ly?\",\n                \"type\": \"technical\",\n                \"difficulty\": difficulty,\n                \"category\": \"system_design\"\n            },\n            {\n                \"question\": \"Explain the concept of database normalization.\",\n                \"type\": \"technical\",\n                \"difficulty\": difficulty,\n                \"category\": \"database\"\n            }\n        ][:count]\n    else:\n        return [\n            {\n                \"question\": \"Tell me about a time you had to work under pressure.\",\n                \"type\": \"non-technical\",\n                \"difficulty\": difficulty,\n                \"category\": \"behavioral\"\n            },\n            {\n                \"question\": \"How do you handle conflicts with team members?\",\n                \"type\": \"non-technical\",\n                \"difficulty\": difficulty,\n                \"category\": \"situational\"\n            },\n            {\n                \"question\": \"What motivates you in your work?\",\n                \"type\": \"non-technical\",\n                \"difficulty\": difficulty,\n                \"category\": \"behavioral\"\n            },\n            {\n                \"question\": \"Describe your leadership style.\",\n                \"type\": \"non-technical\",\n                \"difficulty\": difficulty,\n                \"category\": \"leadership\"\n            },\n            {\n                \"question\": \"How do you prioritize tasks when everything seems urgent?\",\n                \"type\": \"non-technical\",\n                \"difficulty\": difficulty,\n                \"category\": \"situational\"\n            }\n        ][:count]\n","size_bytes":10410},"main.py":{"content":"from app import app\nimport routes\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n","size_bytes":113},"models.py":{"content":"from datetime import datetime\nfrom app import db\nfrom flask_login import UserMixin\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass User(UserMixin, db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n    password_hash = db.Column(db.String(256), nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n    \n    # Relationships\n    interviews = db.relationship('Interview', backref='user', lazy=True, cascade='all, delete-orphan')\n    analytics = db.relationship('Analytics', backref='user', lazy=True, cascade='all, delete-orphan')\n\n    def set_password(self, password):\n        self.password_hash = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\n\n    def __repr__(self):\n        return f'<User {self.username}>'\n\nclass Interview(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)\n    interview_type = db.Column(db.String(50), nullable=False)  # 'technical' or 'non-technical'\n    status = db.Column(db.String(20), default='in_progress')  # 'in_progress', 'completed', 'cancelled'\n    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n    completed_at = db.Column(db.DateTime)\n    total_questions = db.Column(db.Integer, default=0)\n    questions_answered = db.Column(db.Integer, default=0)\n    overall_score = db.Column(db.Float)\n    \n    # Relationships\n    questions = db.relationship('Question', backref='interview', lazy=True, cascade='all, delete-orphan')\n    analytics = db.relationship('Analytics', backref='interview', lazy=True, cascade='all, delete-orphan')\n\nclass Question(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    interview_id = db.Column(db.Integer, db.ForeignKey('interview.id'), nullable=False)\n    question_text = db.Column(db.Text, nullable=False)\n    question_type = db.Column(db.String(50), nullable=False)\n    difficulty_level = db.Column(db.String(20), nullable=False)  # 'easy', 'medium', 'hard'\n    user_answer = db.Column(db.Text)\n    audio_file_path = db.Column(db.String(255))\n    response_time = db.Column(db.Float)  # in seconds\n    confidence_score = db.Column(db.Float)\n    technical_accuracy = db.Column(db.Float)\n    communication_score = db.Column(db.Float)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n    answered_at = db.Column(db.DateTime)\n    \n    # AI Analysis\n    ai_feedback = db.Column(db.Text)\n    improvement_suggestions = db.Column(db.Text)\n\nclass Analytics(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)\n    interview_id = db.Column(db.Integer, db.ForeignKey('interview.id'), nullable=False)\n    \n    # Performance Metrics\n    overall_performance = db.Column(db.Float)\n    technical_skills_score = db.Column(db.Float)\n    communication_skills_score = db.Column(db.Float)\n    confidence_level = db.Column(db.Float)\n    \n    # Face Analysis Metrics\n    eye_contact_percentage = db.Column(db.Float)\n    facial_expression_score = db.Column(db.Float)\n    posture_score = db.Column(db.Float)\n    nervousness_indicators = db.Column(db.Float)\n    \n    # Speech Analysis\n    speech_clarity = db.Column(db.Float)\n    speech_pace = db.Column(db.Float)\n    filler_words_count = db.Column(db.Integer)\n    \n    # Time Analytics\n    average_response_time = db.Column(db.Float)\n    total_interview_duration = db.Column(db.Float)\n    \n    # Improvement Areas\n    strengths = db.Column(db.Text)\n    areas_for_improvement = db.Column(db.Text)\n    detailed_feedback = db.Column(db.Text)\n    \n    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n\nclass FaceAnalysis(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    interview_id = db.Column(db.Integer, db.ForeignKey('interview.id'), nullable=False)\n    timestamp = db.Column(db.Float, nullable=False)  # timestamp in interview\n    \n    # Emotion Detection\n    happiness = db.Column(db.Float, default=0.0)\n    confidence = db.Column(db.Float, default=0.0)\n    nervousness = db.Column(db.Float, default=0.0)\n    concentration = db.Column(db.Float, default=0.0)\n    \n    # Eye Contact\n    looking_at_camera = db.Column(db.Boolean, default=False)\n    \n    # Head Position\n    head_position_x = db.Column(db.Float, default=0.0)\n    head_position_y = db.Column(db.Float, default=0.0)\n    \n    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n","size_bytes":4674},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"email-validator>=2.2.0\",\n    \"flask-dance>=7.1.0\",\n    \"flask>=3.1.1\",\n    \"flask-sqlalchemy>=3.1.1\",\n    \"google-genai>=1.31.0\",\n    \"gunicorn>=23.0.0\",\n    \"psycopg2-binary>=2.9.10\",\n    \"flask-login>=0.6.3\",\n    \"oauthlib>=3.3.1\",\n    \"pyjwt>=2.10.1\",\n    \"numpy>=2.3.2\",\n    \"tensorflow>=2.20.0\",\n    \"opencv-python>=4.11.0.86\",\n    \"pillow>=11.3.0\",\n    \"scikit-learn>=1.7.1\",\n    \"pydantic>=2.11.7\",\n    \"sift-stack-py>=0.8.4\",\n    \"sqlalchemy>=2.0.43\",\n    \"matplotlib>=3.10.5\",\n    \"pandas>=2.3.1\",\n    \"werkzeug>=3.1.3\",\n]\n","size_bytes":679},"replit.md":{"content":"# Overview\n\nInterview Prep AI is a comprehensive interview preparation platform that leverages artificial intelligence to help users practice and improve their interview skills. The application combines LLM-powered question generation, real-time face analysis, speech recognition, and detailed analytics to provide a complete interview simulation experience. The platform supports both technical and non-technical interview types, offering personalized feedback and performance tracking to help users identify strengths and areas for improvement.\n\n# User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n# System Architecture\n\n## Backend Architecture\nThe application uses Flask as the web framework with SQLAlchemy for database operations. The backend follows a traditional MVC pattern with clear separation of concerns:\n\n- **Flask Application Structure**: Core application logic in `app.py` with modular route handling in `routes.py`\n- **Database Layer**: SQLAlchemy ORM with declarative base model for clean database interactions\n- **Authentication**: Flask-Login integration for user session management and route protection\n- **Configuration Management**: Environment-based configuration for database connections and API keys\n\n## Database Design\nThe database schema supports a comprehensive interview tracking system:\n\n- **User Management**: Core user table with authentication and profile information\n- **Interview Sessions**: Hierarchical structure linking users to interviews to individual questions\n- **Analytics Storage**: Dedicated tables for performance metrics and face analysis data\n- **Question Management**: Flexible question storage supporting different types and difficulty levels\n\n## AI Integration Architecture\nThe platform integrates multiple AI services for enhanced functionality:\n\n- **Gemini LLM Service**: Primary AI engine for question generation and response analysis using Google's Gemini API\n- **GAN Service**: Mock implementation for question diversity enhancement (production-ready interface for future GAN integration)\n- **Face Analysis**: Client-side face detection using browser APIs for real-time emotion and engagement tracking\n- **Speech Recognition**: Browser-based speech-to-text for answer transcription and analysis\n\n## Frontend Architecture\nThe frontend uses a responsive design approach with modern web technologies:\n\n- **Template Engine**: Jinja2 templating with Bootstrap for responsive UI components\n- **Real-time Features**: JavaScript modules for webcam access, audio recording, and live analysis\n- **Chart Visualization**: Chart.js integration for analytics dashboard and performance metrics\n- **Progressive Enhancement**: Graceful degradation for browsers without advanced API support\n\n## Security and Session Management\n- **Password Security**: Werkzeug password hashing for secure credential storage\n- **Session Handling**: Flask session management with secure secret key configuration\n- **Proxy Support**: ProxyFix middleware for deployment behind reverse proxies\n- **Database Security**: Connection pooling and ping mechanisms for reliable database connections\n\n# External Dependencies\n\n## AI and Machine Learning Services\n- **Google Gemini API**: Primary LLM service for question generation and response analysis\n- **Browser Face Detection API**: Native browser APIs for real-time face analysis and emotion detection\n- **Web Speech API**: Browser-based speech recognition for answer transcription\n\n## Database and Storage\n- **PostgreSQL**: Primary database for user data, interview sessions, and analytics (configurable via DATABASE_URL)\n- **SQLAlchemy**: ORM layer with connection pooling and health monitoring\n- **File Storage**: Local file system for audio recordings and user uploads\n\n## Frontend Libraries and Frameworks\n- **Bootstrap**: UI framework with dark theme support for responsive design\n- **Font Awesome**: Icon library for consistent visual elements\n- **Chart.js**: JavaScript charting library for analytics visualization\n- **MediaRecorder API**: Browser API for audio/video recording capabilities\n\n## Authentication and Security\n- **Flask-Login**: User session management and authentication flows\n- **Werkzeug**: Security utilities for password hashing and middleware support\n\n## Development and Deployment\n- **Flask Development Server**: Built-in server for development and testing\n- **Replit Environment**: Optimized for Replit deployment with environment variable configuration","size_bytes":4473},"routes.py":{"content":"import os\nimport json\nimport logging\nfrom datetime import datetime\nfrom flask import render_template, request, redirect, url_for, flash, jsonify, session\nfrom flask_login import login_user, logout_user, login_required, current_user\nfrom werkzeug.utils import secure_filename\nfrom app import app, db\nfrom models import User, Interview, Question, Analytics, FaceAnalysis\nfrom gemini_service import generate_interview_questions, analyze_user_response, generate_interview_analytics\nfrom gan_service import MockGANService\n\n# Initialize GAN service\ngan_service = MockGANService()\n\n@app.route('/')\ndef index():\n    \"\"\"Landing page\"\"\"\n    if current_user.is_authenticated:\n        return redirect(url_for('dashboard'))\n    return render_template('index.html')\n\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    \"\"\"User registration\"\"\"\n    if request.method == 'POST':\n        username = request.form['username']\n        email = request.form['email']\n        password = request.form['password']\n        \n        # Check if user already exists\n        if User.query.filter_by(username=username).first():\n            flash('Username already exists', 'error')\n            return render_template('register.html')\n        \n        if User.query.filter_by(email=email).first():\n            flash('Email already registered', 'error')\n            return render_template('register.html')\n        \n        # Create new user\n        user = User()\n        user.username = username\n        user.email = email\n        user.set_password(password)\n        \n        try:\n            db.session.add(user)\n            db.session.commit()\n            flash('Registration successful! Please log in.', 'success')\n            return redirect(url_for('login'))\n        except Exception as e:\n            db.session.rollback()\n            logging.error(f\"Registration error: {e}\")\n            flash('Registration failed. Please try again.', 'error')\n    \n    return render_template('register.html')\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    \"\"\"User login\"\"\"\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n        \n        user = User.query.filter_by(username=username).first()\n        \n        if user and user.check_password(password):\n            login_user(user)\n            next_page = request.args.get('next')\n            return redirect(next_page) if next_page else redirect(url_for('dashboard'))\n        else:\n            flash('Invalid username or password', 'error')\n    \n    return render_template('login.html')\n\n@app.route('/logout')\n@login_required\ndef logout():\n    \"\"\"User logout\"\"\"\n    logout_user()\n    flash('You have been logged out successfully.', 'info')\n    return redirect(url_for('index'))\n\n@app.route('/dashboard')\n@login_required\ndef dashboard():\n    \"\"\"User dashboard\"\"\"\n    # Get user's interview statistics\n    total_interviews = Interview.query.filter_by(user_id=current_user.id).count()\n    completed_interviews = Interview.query.filter_by(user_id=current_user.id, status='completed').count()\n    \n    # Get recent interviews\n    recent_interviews = Interview.query.filter_by(user_id=current_user.id)\\\n        .order_by(Interview.created_at.desc()).limit(5).all()\n    \n    # Calculate average performance\n    analytics = Analytics.query.filter_by(user_id=current_user.id).all()\n    avg_performance = sum(a.overall_performance for a in analytics) / len(analytics) if analytics else 0\n    \n    return render_template('dashboard.html',\n                         total_interviews=total_interviews,\n                         completed_interviews=completed_interviews,\n                         recent_interviews=recent_interviews,\n                         avg_performance=avg_performance)\n\n@app.route('/start_interview', methods=['GET', 'POST'])\n@login_required\ndef start_interview():\n    \"\"\"Start a new interview\"\"\"\n    if request.method == 'POST':\n        interview_type = request.form['interview_type']\n        difficulty = request.form.get('difficulty', 'medium')\n        \n        # Create new interview\n        interview = Interview()\n        interview.user_id = current_user.id\n        interview.interview_type = interview_type\n        interview.status = 'in_progress'\n        \n        try:\n            db.session.add(interview)\n            db.session.commit()\n            \n            # Generate questions using Gemini API\n            base_questions = generate_interview_questions(interview_type, difficulty, 5)\n            \n            # Enhance with GAN diversity\n            diverse_questions = gan_service.generate_diverse_questions(base_questions, diversity_factor=0.4)\n            enhanced_questions = gan_service.enhance_question_quality(diverse_questions)\n            \n            # Save questions to database\n            for q_data in enhanced_questions:\n                question = Question()\n                question.interview_id = interview.id\n                question.question_text = q_data['question']\n                question.question_type = q_data['type']\n                question.difficulty_level = q_data['difficulty']\n                db.session.add(question)\n            \n            interview.total_questions = len(enhanced_questions)\n            db.session.commit()\n            \n            return redirect(url_for('interview_session', interview_id=interview.id))\n            \n        except Exception as e:\n            db.session.rollback()\n            logging.error(f\"Error starting interview: {e}\")\n            flash('Failed to start interview. Please try again.', 'error')\n            return redirect(url_for('dashboard'))\n    \n    return render_template('dashboard.html')\n\n@app.route('/interview/<int:interview_id>')\n@login_required\ndef interview_session(interview_id):\n    \"\"\"Interview session page\"\"\"\n    interview = Interview.query.get_or_404(interview_id)\n    \n    # Check if user owns this interview\n    if interview.user_id != current_user.id:\n        flash('Access denied.', 'error')\n        return redirect(url_for('dashboard'))\n    \n    # Get current question\n    current_question = Question.query.filter_by(interview_id=interview_id, answered_at=None).first()\n    \n    if not current_question:\n        # No more questions, complete interview\n        return redirect(url_for('complete_interview', interview_id=interview_id))\n    \n    # Get question progress\n    answered_questions = Question.query.filter_by(interview_id=interview_id)\\\n        .filter(Question.answered_at.isnot(None)).count()\n    \n    progress = (answered_questions / interview.total_questions) * 100 if interview.total_questions > 0 else 0\n    \n    return render_template('interview.html',\n                         interview=interview,\n                         question=current_question,\n                         progress=progress,\n                         question_number=answered_questions + 1)\n\n@app.route('/submit_answer', methods=['POST'])\n@login_required\ndef submit_answer():\n    \"\"\"Submit answer for current question\"\"\"\n    try:\n        data = request.get_json()\n        question_id = data.get('question_id')\n        user_answer = data.get('answer', '')\n        response_time = data.get('response_time', 0)\n        audio_file_path = data.get('audio_file_path', '')\n        \n        question = Question.query.get_or_404(question_id)\n        \n        # Check if user owns this question\n        if question.interview.user_id != current_user.id:\n            return jsonify({'error': 'Access denied'}), 403\n        \n        # Analyze response using Gemini API\n        analysis = analyze_user_response(\n            question.question_text,\n            user_answer,\n            question.question_type\n        )\n        \n        # Update question with answer and analysis\n        question.user_answer = user_answer\n        question.response_time = response_time\n        question.audio_file_path = audio_file_path\n        question.answered_at = datetime.utcnow()\n        question.confidence_score = analysis['confidence_score']\n        question.technical_accuracy = analysis['technical_accuracy']\n        question.communication_score = analysis['communication_score']\n        question.ai_feedback = analysis['feedback']\n        question.improvement_suggestions = analysis['improvement_suggestions']\n        \n        # Update interview progress\n        interview = question.interview\n        interview.questions_answered += 1\n        \n        db.session.commit()\n        \n        return jsonify({\n            'success': True,\n            'analysis': analysis,\n            'questions_remaining': interview.total_questions - interview.questions_answered\n        })\n        \n    except Exception as e:\n        db.session.rollback()\n        logging.error(f\"Error submitting answer: {e}\")\n        return jsonify({'error': 'Failed to submit answer'}), 500\n\n@app.route('/save_face_analysis', methods=['POST'])\n@login_required\ndef save_face_analysis():\n    \"\"\"Save face analysis data\"\"\"\n    try:\n        data = request.get_json()\n        interview_id = data.get('interview_id')\n        timestamp = data.get('timestamp', 0)\n        analysis_data = data.get('analysis', {})\n        \n        interview = Interview.query.get_or_404(interview_id)\n        \n        # Check if user owns this interview\n        if interview.user_id != current_user.id:\n            return jsonify({'error': 'Access denied'}), 403\n        \n        # Save face analysis\n        face_analysis = FaceAnalysis()\n        face_analysis.interview_id = interview_id\n        face_analysis.timestamp = timestamp\n        face_analysis.happiness = analysis_data.get('happiness', 0.0)\n        face_analysis.confidence = analysis_data.get('confidence', 0.0)\n        face_analysis.nervousness = analysis_data.get('nervousness', 0.0)\n        face_analysis.concentration = analysis_data.get('concentration', 0.0)\n        face_analysis.looking_at_camera = analysis_data.get('looking_at_camera', False)\n        face_analysis.head_position_x = analysis_data.get('head_position_x', 0.0)\n        face_analysis.head_position_y = analysis_data.get('head_position_y', 0.0)\n        \n        db.session.add(face_analysis)\n        db.session.commit()\n        \n        return jsonify({'success': True})\n        \n    except Exception as e:\n        db.session.rollback()\n        logging.error(f\"Error saving face analysis: {e}\")\n        return jsonify({'error': 'Failed to save face analysis'}), 500\n\n@app.route('/complete_interview/<int:interview_id>')\n@login_required\ndef complete_interview(interview_id):\n    \"\"\"Complete interview and generate analytics\"\"\"\n    interview = Interview.query.get_or_404(interview_id)\n    \n    # Check if user owns this interview\n    if interview.user_id != current_user.id:\n        flash('Access denied.', 'error')\n        return redirect(url_for('dashboard'))\n    \n    try:\n        # Mark interview as completed\n        interview.status = 'completed'\n        interview.completed_at = datetime.utcnow()\n        \n        # Get questions and face analysis data\n        questions = Question.query.filter_by(interview_id=interview_id).all()\n        face_analysis_data = FaceAnalysis.query.filter_by(interview_id=interview_id).all()\n        \n        # Prepare data for analytics generation\n        questions_data = []\n        for q in questions:\n            questions_data.append({\n                'question': q.question_text,\n                'answer': q.user_answer,\n                'response_time': q.response_time,\n                'confidence_score': q.confidence_score,\n                'technical_accuracy': q.technical_accuracy,\n                'communication_score': q.communication_score\n            })\n        \n        face_data = []\n        for f in face_analysis_data:\n            face_data.append({\n                'timestamp': f.timestamp,\n                'happiness': f.happiness,\n                'confidence': f.confidence,\n                'nervousness': f.nervousness,\n                'concentration': f.concentration,\n                'looking_at_camera': f.looking_at_camera\n            })\n        \n        # Generate comprehensive analytics using Gemini API\n        analytics_result = generate_interview_analytics(questions_data, face_data)\n        \n        # Calculate additional metrics\n        total_response_time = sum(q.response_time for q in questions if q.response_time)\n        avg_response_time = total_response_time / len(questions) if questions else 0\n        \n        # Calculate face analysis metrics\n        if face_analysis_data:\n            eye_contact_percentage = sum(1 for f in face_analysis_data if f.looking_at_camera) / len(face_analysis_data)\n            avg_nervousness = sum(f.nervousness for f in face_analysis_data) / len(face_analysis_data)\n            avg_confidence = sum(f.confidence for f in face_analysis_data) / len(face_analysis_data)\n        else:\n            eye_contact_percentage = 0.5\n            avg_nervousness = 0.3\n            avg_confidence = 0.6\n        \n        # Create analytics record\n        analytics = Analytics()\n        analytics.user_id = current_user.id\n        analytics.interview_id = interview_id\n        analytics.overall_performance = analytics_result['overall_performance']\n        analytics.technical_skills_score = analytics_result['technical_skills_score']\n        analytics.communication_skills_score = analytics_result['communication_skills_score']\n        analytics.confidence_level = analytics_result['confidence_level']\n        analytics.eye_contact_percentage = eye_contact_percentage\n        analytics.facial_expression_score = avg_confidence\n        analytics.nervousness_indicators = avg_nervousness\n        analytics.average_response_time = avg_response_time\n        analytics.total_interview_duration = total_response_time\n        analytics.strengths = analytics_result['strengths']\n        analytics.areas_for_improvement = analytics_result['areas_for_improvement']\n        analytics.detailed_feedback = analytics_result['detailed_feedback']\n        \n        # Calculate overall score for interview\n        interview.overall_score = analytics_result['overall_performance']\n        \n        db.session.add(analytics)\n        db.session.commit()\n        \n        flash('Interview completed successfully!', 'success')\n        return redirect(url_for('view_analytics', interview_id=interview_id))\n        \n    except Exception as e:\n        db.session.rollback()\n        logging.error(f\"Error completing interview: {e}\")\n        flash('Error completing interview. Please try again.', 'error')\n        return redirect(url_for('dashboard'))\n\n@app.route('/analytics/<int:interview_id>')\n@login_required\ndef view_analytics(interview_id):\n    \"\"\"View interview analytics\"\"\"\n    interview = Interview.query.get_or_404(interview_id)\n    \n    # Check if user owns this interview\n    if interview.user_id != current_user.id:\n        flash('Access denied.', 'error')\n        return redirect(url_for('dashboard'))\n    \n    analytics = Analytics.query.filter_by(interview_id=interview_id).first()\n    if not analytics:\n        flash('Analytics not available for this interview.', 'error')\n        return redirect(url_for('dashboard'))\n    \n    # Get questions for detailed review\n    questions = Question.query.filter_by(interview_id=interview_id).all()\n    \n    # Get face analysis data for visualization\n    face_analysis = FaceAnalysis.query.filter_by(interview_id=interview_id).all()\n    \n    return render_template('analytics.html',\n                         interview=interview,\n                         analytics=analytics,\n                         questions=questions,\n                         face_analysis=face_analysis)\n\n@app.route('/analytics')\n@login_required\ndef analytics_history():\n    \"\"\"View all analytics history\"\"\"\n    user_analytics = Analytics.query.filter_by(user_id=current_user.id)\\\n        .join(Interview).order_by(Interview.created_at.desc()).all()\n    \n    return render_template('analytics.html', analytics_list=user_analytics)\n\n@app.route('/profile')\n@login_required\ndef profile():\n    \"\"\"User profile page\"\"\"\n    # Get user statistics\n    total_interviews = Interview.query.filter_by(user_id=current_user.id).count()\n    completed_interviews = Interview.query.filter_by(user_id=current_user.id, status='completed').count()\n    \n    # Get performance trends\n    analytics = Analytics.query.filter_by(user_id=current_user.id)\\\n        .join(Interview).order_by(Interview.created_at.asc()).all()\n    \n    performance_trend = [a.overall_performance for a in analytics]\n    \n    return render_template('profile.html',\n                         user=current_user,\n                         total_interviews=total_interviews,\n                         completed_interviews=completed_interviews,\n                         performance_trend=performance_trend)\n\n@app.errorhandler(404)\ndef not_found_error(error):\n    return render_template('404.html'), 404\n\n@app.errorhandler(500)\ndef internal_error(error):\n    db.session.rollback()\n    return render_template('500.html'), 500\n","size_bytes":17045},"static/css/custom.css":{"content":"/* Basic CSS for Interview Practice App */\n/* Simple styling for college project */\n\nbody {\n    font-family: Arial, sans-serif;\n    background-color: #f8f9fa;\n    color: #212529 !important;\n}\n\n/* Make all text darker and more readable */\nh1, h2, h3, h4, h5, h6 {\n    color: #212529 !important;\n    font-weight: bold;\n}\n\np, span, div, li, td, th, label, a {\n    color: #212529 !important;\n}\n\n/* Specific overrides for common elements */\n.btn {\n    font-weight: bold;\n}\n\n.form-label {\n    color: #212529 !important;\n    font-weight: bold;\n    margin-bottom: 8px;\n    display: block;\n}\n\n/* Input group styling */\n.input-group-text {\n    background-color: #e9ecef;\n    color: #212529 !important;\n    border: 1px solid #ccc;\n}\n\n/* Button styling for forms */\n.btn-primary {\n    background-color: #007bff;\n    border-color: #007bff;\n    color: white !important;\n    font-weight: bold;\n}\n\n.btn-secondary {\n    background-color: #6c757d;\n    border-color: #6c757d;\n    color: white !important;\n    font-weight: bold;\n}\n\n.btn-success {\n    background-color: #28a745;\n    border-color: #28a745;\n    color: white !important;\n    font-weight: bold;\n}\n\n.list-group-item {\n    color: #212529 !important;\n    background-color: #ffffff;\n}\n\n/* Analytics and dashboard specific */\n.metric-value {\n    color: #212529 !important;\n    font-weight: bold;\n    font-size: 18px;\n}\n\n.timeline-content {\n    color: #212529 !important;\n}\n\n/* Interview results and feedback */\n.feedback-text {\n    color: #212529 !important;\n}\n\n.score-text {\n    color: #212529 !important;\n    font-weight: bold;\n}\n\n/* Form validation states */\n.is-valid {\n    border-color: #28a745 !important;\n}\n\n.is-invalid {\n    border-color: #dc3545 !important;\n}\n\n.valid-feedback {\n    color: #28a745 !important;\n    font-weight: bold;\n}\n\n.invalid-feedback {\n    color: #dc3545 !important;\n    font-weight: bold;\n}\n\n/* Ensure all form text is visible */\n.form-text {\n    color: #6c757d !important;\n}\n\n/* Radio buttons and checkboxes */\n.form-check-label {\n    color: #212529 !important;\n}\n\n.form-check-input {\n    background-color: white;\n    border-color: #ccc;\n}\n\n.form-check-input:checked {\n    background-color: #007bff;\n    border-color: #007bff;\n}\n\n.text-muted {\n    color: #6c757d !important;\n}\n\n.navbar-dark .navbar-nav .nav-link {\n    color: #ffffff !important;\n}\n\n.navbar-dark .navbar-brand {\n    color: #ffffff !important;\n}\n\n.navbar-brand {\n    font-weight: bold;\n}\n\n.card {\n    border: 1px solid #ddd;\n    margin-bottom: 20px;\n    background-color: #ffffff;\n}\n\n.card-header {\n    background-color: #e9ecef;\n    font-weight: bold;\n    padding: 15px;\n    color: #212529 !important;\n}\n\n.card-body {\n    background-color: #ffffff;\n    color: #212529 !important;\n}\n\n.card-title {\n    color: #212529 !important;\n    font-weight: bold;\n}\n\n.card-text {\n    color: #212529 !important;\n}\n\n.btn {\n    padding: 8px 16px;\n    margin: 5px;\n}\n\n.video-container {\n    background-color: #333;\n    padding: 10px;\n    min-height: 400px;\n    text-align: center;\n}\n\n.face-analysis-overlay {\n    background-color: rgba(0, 0, 0, 0.8);\n    color: #ffffff !important;\n    padding: 10px;\n    font-size: 14px;\n    font-weight: bold;\n}\n\n.recording-indicator {\n    background-color: red;\n    color: white;\n    padding: 5px 10px;\n    display: inline-block;\n    margin: 10px;\n}\n\n.progress {\n    height: 20px;\n    background-color: #e9ecef;\n    border: 1px solid #ccc;\n}\n\n.progress-bar {\n    background-color: #007bff;\n}\n\n.metric-card {\n    background-color: white;\n    border: 1px solid #ddd;\n    padding: 15px;\n    margin: 10px 0;\n}\n\n.score-circle {\n    border: 2px solid #007bff;\n    border-radius: 50%;\n    padding: 10px;\n    text-align: center;\n    display: inline-block;\n}\n\n.strength-area {\n    background-color: #d4edda;\n    padding: 10px;\n    border: 1px solid #c3e6cb;\n    margin: 10px 0;\n}\n\n.improvement-area {\n    background-color: #fff3cd;\n    padding: 10px;\n    border: 1px solid #ffeaa7;\n    margin: 10px 0;\n}\n\n.chart-container {\n    background-color: white;\n    border: 1px solid #ddd;\n    padding: 15px;\n    margin: 20px 0;\n}\n\n.question-card {\n    background-color: #e7f1ff;\n    border: 1px solid #b8daff;\n    padding: 15px;\n    margin: 15px 0;\n}\n\n.question-text {\n    font-size: 16px;\n    font-weight: bold;\n    margin: 10px 0;\n    color: #212529 !important;\n}\n\n.audio-level-container {\n    background-color: #f8f9fa;\n    border: 1px solid #ddd;\n    padding: 10px;\n}\n\n.audio-level-bar {\n    background-color: #28a745;\n    height: 20px;\n}\n\n.live-analysis-container {\n    background-color: #f8f9fa;\n    border: 1px solid #ddd;\n    padding: 15px;\n    margin: 10px 0;\n}\n\n.analysis-bar {\n    height: 15px;\n    background-color: #007bff;\n    margin: 5px 0;\n}\n\n.tips-container {\n    background-color: #d1ecf1;\n    border: 1px solid #bee5eb;\n    padding: 15px;\n    margin: 15px 0;\n}\n\n.tip-item {\n    padding: 8px 0;\n    border-bottom: 1px solid #ccc;\n}\n\n.tip-item:last-child {\n    border-bottom: none;\n}\n\n.toast {\n    border: 1px solid #ccc;\n    background-color: white;\n}\n\n.timeline-container {\n    padding-left: 30px;\n}\n\n.timeline-item {\n    margin-bottom: 20px;\n    padding: 10px;\n    border: 1px solid #ddd;\n    background-color: white;\n}\n\n.profile-image {\n    border: 2px solid #007bff;\n    border-radius: 5px;\n}\n\n.quick-action-card {\n    cursor: pointer;\n    background-color: #f8f9fa;\n    border: 1px solid #ddd;\n    padding: 15px;\n    text-align: center;\n}\n\n.stat-icon {\n    font-size: 24px;\n}\n\n.stat-card {\n    text-align: center;\n    padding: 20px;\n    background-color: white;\n    border: 1px solid #ddd;\n    margin: 10px;\n}\n\n.form-control,\n.form-select {\n    border: 1px solid #ccc;\n    background-color: white;\n    padding: 8px;\n    margin: 5px 0;\n    color: #212529 !important;\n    font-weight: normal;\n}\n\n.form-control:focus,\n.form-select:focus {\n    border-color: #007bff;\n    outline: none;\n    background-color: white;\n    color: #212529 !important;\n}\n\n/* Dropdown options styling */\n.form-select option {\n    background-color: white !important;\n    color: #212529 !important;\n    padding: 8px;\n    font-weight: normal;\n}\n\n/* Placeholder styling */\n.form-control::placeholder {\n    color: #6c757d !important;\n    opacity: 1;\n    font-weight: normal;\n}\n\n.form-select:invalid {\n    color: #6c757d !important;\n}\n\n/* When dropdown is opened */\nselect {\n    background-color: white !important;\n    color: #212529 !important;\n}\n\nselect option {\n    background-color: white !important;\n    color: #212529 !important;\n    padding: 10px;\n}\n\nselect option:checked {\n    background-color: #007bff !important;\n    color: white !important;\n}\n\nselect option:hover {\n    background-color: #f8f9fa !important;\n    color: #212529 !important;\n}\n\n.alert {\n    border: 1px solid;\n    padding: 10px;\n    margin: 10px 0;\n}\n\n.alert-success {\n    background-color: #d4edda;\n    border-color: #c3e6cb;\n    color: #155724 !important;\n    font-weight: bold;\n}\n\n.alert-danger {\n    background-color: #f8d7da;\n    border-color: #f5c6cb;\n    color: #721c24 !important;\n    font-weight: bold;\n}\n\n.alert-warning {\n    background-color: #fff3cd;\n    border-color: #ffeaa7;\n    color: #856404 !important;\n    font-weight: bold;\n}\n\n.alert-info {\n    background-color: #d1ecf1;\n    border-color: #bee5eb;\n    color: #0c5460 !important;\n    font-weight: bold;\n}\n\n/* Simple responsive design */\n@media (max-width: 768px) {\n    .container {\n        padding: 10px;\n    }\n    \n    .card {\n        margin: 10px 0;\n    }\n    \n    .btn {\n        display: block;\n        margin: 10px 0;\n        width: 100%;\n    }\n}\n\n/* Basic table styling */\ntable {\n    width: 100%;\n    border-collapse: collapse;\n    margin: 20px 0;\n    background-color: #ffffff;\n}\n\ntable th,\ntable td {\n    border: 1px solid #ddd;\n    padding: 8px;\n    text-align: left;\n    color: #212529 !important;\n    background-color: #ffffff;\n}\n\ntable th {\n    background-color: #f2f2f2;\n    font-weight: bold;\n    color: #212529 !important;\n}\n\n/* Bootstrap table overrides */\n.table {\n    color: #212529 !important;\n    background-color: #ffffff;\n}\n\n.table th {\n    color: #212529 !important;\n    background-color: #f8f9fa;\n    font-weight: bold;\n    border-color: #dee2e6;\n}\n\n.table td {\n    color: #212529 !important;\n    background-color: #ffffff;\n    border-color: #dee2e6;\n}\n\n.table-striped tbody tr:nth-of-type(odd) {\n    background-color: #f8f9fa;\n    color: #212529 !important;\n}\n\n.table-striped tbody tr:nth-of-type(odd) td {\n    color: #212529 !important;\n}\n\n.table-hover tbody tr:hover {\n    background-color: #e9ecef;\n    color: #212529 !important;\n}\n\n.table-hover tbody tr:hover td {\n    color: #212529 !important;\n}\n\n/* Simple footer */\n.footer {\n    background-color: #343a40;\n    color: white !important;\n    text-align: center;\n    padding: 20px;\n    margin-top: 40px;\n}\n\n.footer * {\n    color: white !important;\n}\n\n.footer p {\n    color: white !important;\n    margin: 0;\n}\n\n.footer a {\n    color: #ffffff !important;\n    text-decoration: underline;\n}\n\n.footer a:hover {\n    color: #f8f9fa !important;\n}\n\n/* Fix any footer content that might be dark */\n.bg-dark,\n.bg-primary {\n    color: white !important;\n}\n\n.bg-dark *,\n.bg-primary * {\n    color: white !important;\n}\n\n/* Navbar footer or any dark backgrounds */\n.navbar-dark {\n    background-color: #343a40 !important;\n}\n\n.navbar-dark * {\n    color: white !important;\n}\n","size_bytes":9306},"static/js/analytics.js":{"content":"/**\n * Analytics Dashboard Charts and Visualizations\n * Uses Chart.js for data visualization\n */\n\n/**\n * Initialize all analytics charts\n */\nfunction initializeAnalyticsCharts(analyticsData, faceAnalysisData) {\n    initializePerformanceChart(analyticsData);\n    initializeFaceAnalysisChart(faceAnalysisData);\n}\n\n/**\n * Performance Breakdown Radar Chart\n */\nfunction initializePerformanceChart(data) {\n    const ctx = document.getElementById('performanceChart');\n    if (!ctx) return;\n    \n    new Chart(ctx.getContext('2d'), {\n        type: 'radar',\n        data: {\n            labels: [\n                'Overall Performance',\n                'Technical Skills',\n                'Communication',\n                'Confidence',\n                'Eye Contact',\n                'Facial Expression'\n            ],\n            datasets: [{\n                label: 'Your Performance',\n                data: [\n                    data.overall * 100,\n                    data.technical * 100,\n                    data.communication * 100,\n                    data.confidence * 100,\n                    data.eyeContact * 100,\n                    data.facialExpression * 100\n                ],\n                backgroundColor: 'rgba(54, 162, 235, 0.2)',\n                borderColor: 'rgba(54, 162, 235, 1)',\n                borderWidth: 2,\n                pointBackgroundColor: 'rgba(54, 162, 235, 1)',\n                pointBorderColor: '#fff',\n                pointHoverBackgroundColor: '#fff',\n                pointHoverBorderColor: 'rgba(54, 162, 235, 1)'\n            }]\n        },\n        options: {\n            responsive: true,\n            maintainAspectRatio: false,\n            plugins: {\n                legend: {\n                    display: false\n                }\n            },\n            scales: {\n                r: {\n                    beginAtZero: true,\n                    max: 100,\n                    ticks: {\n                        stepSize: 20,\n                        callback: function(value) {\n                            return value + '%';\n                        }\n                    },\n                    grid: {\n                        color: 'rgba(255, 255, 255, 0.1)'\n                    },\n                    angleLines: {\n                        color: 'rgba(255, 255, 255, 0.1)'\n                    }\n                }\n            }\n        }\n    });\n}\n\n/**\n * Face Analysis Timeline Chart\n */\nfunction initializeFaceAnalysisChart(faceData) {\n    const ctx = document.getElementById('faceAnalysisChart');\n    if (!ctx || !faceData || faceData.length === 0) return;\n    \n    // Prepare timeline data\n    const timestamps = faceData.map(d => formatTimestamp(d.timestamp));\n    const confidenceData = faceData.map(d => d.confidence * 100);\n    const happinessData = faceData.map(d => d.happiness * 100);\n    const concentrationData = faceData.map(d => d.concentration * 100);\n    const nervousnessData = faceData.map(d => d.nervousness * 100);\n    \n    new Chart(ctx.getContext('2d'), {\n        type: 'line',\n        data: {\n            labels: timestamps,\n            datasets: [\n                {\n                    label: 'Confidence',\n                    data: confidenceData,\n                    borderColor: 'rgb(75, 192, 192)',\n                    backgroundColor: 'rgba(75, 192, 192, 0.2)',\n                    tension: 0.4\n                },\n                {\n                    label: 'Happiness',\n                    data: happinessData,\n                    borderColor: 'rgb(255, 205, 86)',\n                    backgroundColor: 'rgba(255, 205, 86, 0.2)',\n                    tension: 0.4\n                },\n                {\n                    label: 'Concentration',\n                    data: concentrationData,\n                    borderColor: 'rgb(54, 162, 235)',\n                    backgroundColor: 'rgba(54, 162, 235, 0.2)',\n                    tension: 0.4\n                },\n                {\n                    label: 'Nervousness',\n                    data: nervousnessData,\n                    borderColor: 'rgb(255, 99, 132)',\n                    backgroundColor: 'rgba(255, 99, 132, 0.2)',\n                    tension: 0.4\n                }\n            ]\n        },\n        options: {\n            responsive: true,\n            maintainAspectRatio: false,\n            plugins: {\n                legend: {\n                    position: 'bottom',\n                    labels: {\n                        usePointStyle: true,\n                        padding: 20\n                    }\n                }\n            },\n            scales: {\n                y: {\n                    beginAtZero: true,\n                    max: 100,\n                    ticks: {\n                        callback: function(value) {\n                            return value + '%';\n                        }\n                    },\n                    grid: {\n                        color: 'rgba(255, 255, 255, 0.1)'\n                    }\n                },\n                x: {\n                    grid: {\n                        color: 'rgba(255, 255, 255, 0.1)'\n                    }\n                }\n            },\n            interaction: {\n                intersect: false,\n                mode: 'index'\n            }\n        }\n    });\n}\n\n/**\n * Performance History Chart for Profile Page\n */\nfunction initializePerformanceHistoryChart(performanceData, elementId = 'performanceHistoryChart') {\n    const ctx = document.getElementById(elementId);\n    if (!ctx || !performanceData || performanceData.length === 0) return;\n    \n    // Calculate moving average\n    const movingAverage = calculateMovingAverage(performanceData, 3);\n    \n    new Chart(ctx.getContext('2d'), {\n        type: 'line',\n        data: {\n            labels: performanceData.map((_, index) => `Interview ${index + 1}`),\n            datasets: [\n                {\n                    label: 'Performance Score',\n                    data: performanceData.map(score => score * 100),\n                    borderColor: 'rgb(75, 192, 192)',\n                    backgroundColor: 'rgba(75, 192, 192, 0.2)',\n                    tension: 0.4,\n                    fill: true,\n                    pointRadius: 6,\n                    pointHoverRadius: 8\n                },\n                {\n                    label: 'Trend',\n                    data: movingAverage.map(score => score * 100),\n                    borderColor: 'rgb(255, 99, 132)',\n                    backgroundColor: 'transparent',\n                    borderDash: [5, 5],\n                    tension: 0.4,\n                    pointRadius: 0\n                }\n            ]\n        },\n        options: {\n            responsive: true,\n            maintainAspectRatio: false,\n            plugins: {\n                legend: {\n                    display: true,\n                    position: 'top'\n                }\n            },\n            scales: {\n                y: {\n                    beginAtZero: true,\n                    max: 100,\n                    ticks: {\n                        callback: function(value) {\n                            return value + '%';\n                        }\n                    },\n                    grid: {\n                        color: 'rgba(255, 255, 255, 0.1)'\n                    }\n                },\n                x: {\n                    grid: {\n                        color: 'rgba(255, 255, 255, 0.1)'\n                    }\n                }\n            }\n        }\n    });\n}\n\n/**\n * Skills Comparison Chart\n */\nfunction initializeSkillsComparisonChart(skillsData, elementId = 'skillsComparisonChart') {\n    const ctx = document.getElementById(elementId);\n    if (!ctx || !skillsData) return;\n    \n    new Chart(ctx.getContext('2d'), {\n        type: 'doughnut',\n        data: {\n            labels: ['Technical Skills', 'Communication', 'Confidence', 'Presentation'],\n            datasets: [{\n                data: [\n                    skillsData.technical * 100,\n                    skillsData.communication * 100,\n                    skillsData.confidence * 100,\n                    skillsData.presentation * 100\n                ],\n                backgroundColor: [\n                    'rgba(54, 162, 235, 0.8)',\n                    'rgba(75, 192, 192, 0.8)',\n                    'rgba(255, 205, 86, 0.8)',\n                    'rgba(255, 99, 132, 0.8)'\n                ],\n                borderColor: [\n                    'rgb(54, 162, 235)',\n                    'rgb(75, 192, 192)',\n                    'rgb(255, 205, 86)',\n                    'rgb(255, 99, 132)'\n                ],\n                borderWidth: 2\n            }]\n        },\n        options: {\n            responsive: true,\n            maintainAspectRatio: false,\n            plugins: {\n                legend: {\n                    position: 'bottom',\n                    labels: {\n                        padding: 20,\n                        usePointStyle: true\n                    }\n                },\n                tooltip: {\n                    callbacks: {\n                        label: function(context) {\n                            return context.label + ': ' + Math.round(context.parsed) + '%';\n                        }\n                    }\n                }\n            }\n        }\n    });\n}\n\n/**\n * Progress Over Time Chart\n */\nfunction initializeProgressChart(progressData, elementId = 'progressChart') {\n    const ctx = document.getElementById(elementId);\n    if (!ctx || !progressData) return;\n    \n    new Chart(ctx.getContext('2d'), {\n        type: 'bar',\n        data: {\n            labels: progressData.labels,\n            datasets: [{\n                label: 'Average Score',\n                data: progressData.scores,\n                backgroundColor: function(context) {\n                    const value = context.parsed.y;\n                    if (value >= 80) return 'rgba(40, 167, 69, 0.8)';\n                    if (value >= 60) return 'rgba(255, 193, 7, 0.8)';\n                    return 'rgba(220, 53, 69, 0.8)';\n                },\n                borderColor: function(context) {\n                    const value = context.parsed.y;\n                    if (value >= 80) return 'rgb(40, 167, 69)';\n                    if (value >= 60) return 'rgb(255, 193, 7)';\n                    return 'rgb(220, 53, 69)';\n                },\n                borderWidth: 2\n            }]\n        },\n        options: {\n            responsive: true,\n            maintainAspectRatio: false,\n            plugins: {\n                legend: {\n                    display: false\n                }\n            },\n            scales: {\n                y: {\n                    beginAtZero: true,\n                    max: 100,\n                    ticks: {\n                        callback: function(value) {\n                            return value + '%';\n                        }\n                    },\n                    grid: {\n                        color: 'rgba(255, 255, 255, 0.1)'\n                    }\n                },\n                x: {\n                    grid: {\n                        color: 'rgba(255, 255, 255, 0.1)'\n                    }\n                }\n            }\n        }\n    });\n}\n\n/**\n * Utility Functions\n */\n\nfunction formatTimestamp(seconds) {\n    const minutes = Math.floor(seconds / 60);\n    const remainingSeconds = Math.floor(seconds % 60);\n    return `${minutes}:${remainingSeconds.toString().padStart(2, '0')}`;\n}\n\nfunction calculateMovingAverage(data, windowSize) {\n    const result = [];\n    for (let i = 0; i < data.length; i++) {\n        const start = Math.max(0, i - windowSize + 1);\n        const end = i + 1;\n        const window = data.slice(start, end);\n        const average = window.reduce((sum, val) => sum + val, 0) / window.length;\n        result.push(average);\n    }\n    return result;\n}\n\nfunction getPerformanceColor(score) {\n    if (score >= 0.8) return 'success';\n    if (score >= 0.6) return 'warning';\n    return 'danger';\n}\n\nfunction getPerformanceLabel(score) {\n    if (score >= 0.8) return 'Excellent';\n    if (score >= 0.6) return 'Good';\n    if (score >= 0.4) return 'Fair';\n    return 'Needs Improvement';\n}\n\n// Export functions for global use\nwindow.AnalyticsCharts = {\n    initializeAnalyticsCharts,\n    initializePerformanceChart,\n    initializeFaceAnalysisChart,\n    initializePerformanceHistoryChart,\n    initializeSkillsComparisonChart,\n    initializeProgressChart,\n    formatTimestamp,\n    calculateMovingAverage,\n    getPerformanceColor,\n    getPerformanceLabel\n};\n","size_bytes":12514},"static/js/face-detection.js":{"content":"/**\n * Face Detection and Analysis Module\n * Provides face detection, emotion analysis, and eye tracking capabilities\n */\n\nclass FaceAnalyzer {\n    constructor(videoElement) {\n        this.videoElement = videoElement;\n        this.canvas = null;\n        this.ctx = null;\n        this.faceDetector = null;\n        this.isAnalyzing = false;\n        this.analysisInterval = null;\n        \n        // Analysis callbacks\n        this.onFaceAnalysis = null;\n        \n        this.setupCanvas();\n        this.initializeFaceDetection();\n    }\n    \n    setupCanvas() {\n        // Create hidden canvas for analysis\n        this.canvas = document.createElement('canvas');\n        this.ctx = this.canvas.getContext('2d');\n        this.canvas.style.display = 'none';\n        document.body.appendChild(this.canvas);\n    }\n    \n    async initializeFaceDetection() {\n        try {\n            // Check if native Face Detection API is available\n            if ('FaceDetector' in window) {\n                this.faceDetector = new FaceDetector({\n                    fastMode: false,\n                    maxDetectedFaces: 1\n                });\n                console.log('Native face detection initialized');\n            } else {\n                console.warn('Native face detection not available, using fallback');\n            }\n        } catch (error) {\n            console.warn('Face detection initialization failed:', error);\n        }\n    }\n    \n    startAnalysis(callback) {\n        if (this.isAnalyzing) return;\n        \n        this.isAnalyzing = true;\n        this.onFaceAnalysis = callback;\n        \n        // Start analysis loop\n        this.analysisInterval = setInterval(() => {\n            this.analyzeFace();\n        }, 2000); // Analyze every 2 seconds\n        \n        console.log('Face analysis started');\n    }\n    \n    stopAnalysis() {\n        this.isAnalyzing = false;\n        \n        if (this.analysisInterval) {\n            clearInterval(this.analysisInterval);\n            this.analysisInterval = null;\n        }\n        \n        console.log('Face analysis stopped');\n    }\n    \n    async analyzeFace() {\n        if (!this.isAnalyzing || !this.videoElement.videoWidth) return;\n        \n        try {\n            // Update canvas size to match video\n            this.canvas.width = this.videoElement.videoWidth;\n            this.canvas.height = this.videoElement.videoHeight;\n            \n            // Draw current frame to canvas\n            this.ctx.drawImage(this.videoElement, 0, 0);\n            \n            let faceData = null;\n            \n            // Try native face detection first\n            if (this.faceDetector) {\n                try {\n                    const faces = await this.faceDetector.detect(this.canvas);\n                    if (faces.length > 0) {\n                        faceData = this.processFaceDetectionResults(faces[0]);\n                    }\n                } catch (error) {\n                    console.warn('Native face detection failed:', error);\n                }\n            }\n            \n            // Fallback to simulated analysis\n            if (!faceData) {\n                faceData = this.simulateFaceAnalysis();\n            }\n            \n            // Call callback with analysis results\n            if (this.onFaceAnalysis && faceData) {\n                this.onFaceAnalysis(faceData);\n            }\n            \n        } catch (error) {\n            console.error('Face analysis error:', error);\n        }\n    }\n    \n    processFaceDetectionResults(face) {\n        // Process native face detection results\n        const boundingBox = face.boundingBox;\n        const landmarks = face.landmarks || [];\n        \n        // Calculate face center relative to video center\n        const videoCenterX = this.canvas.width / 2;\n        const videoCenterY = this.canvas.height / 2;\n        const faceCenterX = boundingBox.x + boundingBox.width / 2;\n        const faceCenterY = boundingBox.y + boundingBox.height / 2;\n        \n        // Normalize position (-1 to 1)\n        const headPositionX = (faceCenterX - videoCenterX) / videoCenterX;\n        const headPositionY = (faceCenterY - videoCenterY) / videoCenterY;\n        \n        // Estimate eye contact based on face position and size\n        const faceSize = Math.max(boundingBox.width, boundingBox.height);\n        const relativeFaceSize = faceSize / Math.min(this.canvas.width, this.canvas.height);\n        const isLookingAtCamera = this.estimateEyeContact(headPositionX, headPositionY, relativeFaceSize);\n        \n        // Simulate emotion analysis (would use actual emotion detection in production)\n        const emotions = this.simulateEmotionAnalysis(faceSize, headPositionX, headPositionY);\n        \n        return {\n            confidence: emotions.confidence,\n            happiness: emotions.happiness,\n            nervousness: emotions.nervousness,\n            concentration: emotions.concentration,\n            looking_at_camera: isLookingAtCamera,\n            head_position_x: headPositionX,\n            head_position_y: headPositionY,\n            face_detected: true\n        };\n    }\n    \n    simulateFaceAnalysis() {\n        // Simulate realistic face analysis when native detection is unavailable\n        const baseConfidence = 0.6;\n        const baseHappiness = 0.5;\n        const baseNervousness = 0.3;\n        const baseConcentration = 0.7;\n        \n        // Add some realistic variation\n        const variation = () => (Math.random() - 0.5) * 0.2; // ±0.1\n        \n        return {\n            confidence: Math.max(0, Math.min(1, baseConfidence + variation())),\n            happiness: Math.max(0, Math.min(1, baseHappiness + variation())),\n            nervousness: Math.max(0, Math.min(1, baseNervousness + variation())),\n            concentration: Math.max(0, Math.min(1, baseConcentration + variation())),\n            looking_at_camera: Math.random() > 0.25, // 75% chance of eye contact\n            head_position_x: (Math.random() - 0.5) * 0.3, // Slight head movement\n            head_position_y: (Math.random() - 0.5) * 0.2,\n            face_detected: true\n        };\n    }\n    \n    estimateEyeContact(headPosX, headPosY, faceSize) {\n        // Estimate if person is looking at camera based on head position and face size\n        const centerThreshold = 0.3; // How close to center for good eye contact\n        const sizeThreshold = 0.1; // Minimum face size for reliable detection\n        \n        if (faceSize < sizeThreshold) return false;\n        \n        const distanceFromCenter = Math.sqrt(headPosX * headPosX + headPosY * headPosY);\n        return distanceFromCenter < centerThreshold;\n    }\n    \n    simulateEmotionAnalysis(faceSize, headPosX, headPosY) {\n        // Simulate emotion analysis based on face characteristics\n        // In production, this would use actual emotion detection models\n        \n        const distanceFromCenter = Math.sqrt(headPosX * headPosX + headPosY * headPosY);\n        const faceSizeNormalized = Math.min(faceSize / 200, 1); // Normalize face size\n        \n        // Higher confidence when face is centered and appropriately sized\n        const confidence = Math.max(0.3, 0.9 - distanceFromCenter * 0.5 - (1 - faceSizeNormalized) * 0.3);\n        \n        // Happiness tends to be higher when confident\n        const happiness = Math.max(0.2, confidence * 0.8 + Math.random() * 0.2);\n        \n        // Nervousness inversely related to confidence\n        const nervousness = Math.max(0.1, (1 - confidence) * 0.6 + Math.random() * 0.1);\n        \n        // Concentration based on stability (less head movement = more concentration)\n        const concentration = Math.max(0.4, 0.9 - distanceFromCenter * 0.3);\n        \n        return { confidence, happiness, nervousness, concentration };\n    }\n    \n    // Utility method to get current frame as image data\n    getCurrentFrameData() {\n        if (!this.videoElement.videoWidth) return null;\n        \n        this.canvas.width = this.videoElement.videoWidth;\n        this.canvas.height = this.videoElement.videoHeight;\n        this.ctx.drawImage(this.videoElement, 0, 0);\n        \n        return this.ctx.getImageData(0, 0, this.canvas.width, this.canvas.height);\n    }\n    \n    // Clean up resources\n    destroy() {\n        this.stopAnalysis();\n        \n        if (this.canvas && this.canvas.parentNode) {\n            this.canvas.parentNode.removeChild(this.canvas);\n        }\n        \n        this.canvas = null;\n        this.ctx = null;\n        this.faceDetector = null;\n    }\n}\n\n/**\n * Eye Tracking Utility\n * Provides basic eye tracking capabilities\n */\nclass EyeTracker {\n    constructor() {\n        this.eyePositions = [];\n        this.gazePoints = [];\n        this.isTracking = false;\n    }\n    \n    startTracking(callback) {\n        this.isTracking = true;\n        this.onGazeUpdate = callback;\n        \n        // Simple eye tracking simulation\n        this.trackingInterval = setInterval(() => {\n            if (this.isTracking) {\n                this.simulateGazeTracking();\n            }\n        }, 500);\n    }\n    \n    stopTracking() {\n        this.isTracking = false;\n        \n        if (this.trackingInterval) {\n            clearInterval(this.trackingInterval);\n        }\n    }\n    \n    simulateGazeTracking() {\n        // Simulate gaze tracking data\n        const gazePoint = {\n            x: Math.random() * window.innerWidth,\n            y: Math.random() * window.innerHeight,\n            timestamp: Date.now(),\n            confidence: Math.random() * 0.3 + 0.7 // 70-100% confidence\n        };\n        \n        this.gazePoints.push(gazePoint);\n        \n        // Keep only last 10 gaze points\n        if (this.gazePoints.length > 10) {\n            this.gazePoints.shift();\n        }\n        \n        if (this.onGazeUpdate) {\n            this.onGazeUpdate(gazePoint);\n        }\n    }\n    \n    getAverageGazePosition() {\n        if (this.gazePoints.length === 0) return null;\n        \n        const avgX = this.gazePoints.reduce((sum, point) => sum + point.x, 0) / this.gazePoints.length;\n        const avgY = this.gazePoints.reduce((sum, point) => sum + point.y, 0) / this.gazePoints.length;\n        \n        return { x: avgX, y: avgY };\n    }\n}\n\n/**\n * Pose Estimation Utility\n * Basic body pose estimation for interview analysis\n */\nclass PoseAnalyzer {\n    constructor() {\n        this.poseData = [];\n        this.isAnalyzing = false;\n    }\n    \n    startAnalysis(videoElement, callback) {\n        this.isAnalyzing = true;\n        this.onPoseUpdate = callback;\n        this.videoElement = videoElement;\n        \n        // Simulate pose analysis\n        this.poseInterval = setInterval(() => {\n            if (this.isAnalyzing) {\n                this.analyzePose();\n            }\n        }, 1000);\n    }\n    \n    stopAnalysis() {\n        this.isAnalyzing = false;\n        \n        if (this.poseInterval) {\n            clearInterval(this.poseInterval);\n        }\n    }\n    \n    analyzePose() {\n        // Simulate pose analysis results\n        const poseData = {\n            shoulderAlignment: Math.random() * 0.3 + 0.7, // 70-100%\n            posture: Math.random() * 0.4 + 0.6, // 60-100%\n            handGestures: Math.random() > 0.7 ? 'active' : 'calm',\n            bodyLanguage: Math.random() > 0.8 ? 'open' : 'neutral',\n            timestamp: Date.now()\n        };\n        \n        this.poseData.push(poseData);\n        \n        // Keep only last 20 pose samples\n        if (this.poseData.length > 20) {\n            this.poseData.shift();\n        }\n        \n        if (this.onPoseUpdate) {\n            this.onPoseUpdate(poseData);\n        }\n    }\n    \n    getPostureSummary() {\n        if (this.poseData.length === 0) return null;\n        \n        const avgShoulder = this.poseData.reduce((sum, data) => sum + data.shoulderAlignment, 0) / this.poseData.length;\n        const avgPosture = this.poseData.reduce((sum, data) => sum + data.posture, 0) / this.poseData.length;\n        \n        return {\n            shoulderAlignment: avgShoulder,\n            posture: avgPosture,\n            overallScore: (avgShoulder + avgPosture) / 2\n        };\n    }\n}\n\n// Export classes for global use\nwindow.FaceAnalyzer = FaceAnalyzer;\nwindow.EyeTracker = EyeTracker;\nwindow.PoseAnalyzer = PoseAnalyzer;\n","size_bytes":12242},"static/js/interview.js":{"content":"/**\n * Interview Session Management\n * Handles webcam, audio recording, face analysis, and session flow\n */\n\nclass InterviewSession {\n    constructor(config) {\n        this.questionId = config.questionId;\n        this.interviewId = config.interviewId;\n        this.questionNumber = config.questionNumber;\n        this.totalQuestions = config.totalQuestions;\n        \n        // Media elements\n        this.videoElement = document.getElementById('userVideo');\n        this.stream = null;\n        this.mediaRecorder = null;\n        this.audioChunks = [];\n        this.videoChunks = [];\n        \n        // UI elements\n        this.startBtn = document.getElementById('startRecordingBtn');\n        this.stopBtn = document.getElementById('stopRecordingBtn');\n        this.submitBtn = document.getElementById('submitAnswerBtn');\n        this.recordingStatus = document.getElementById('recordingStatus');\n        this.timer = document.getElementById('timer');\n        this.recordingIndicator = document.getElementById('recordingIndicator');\n        this.audioLevelBar = document.getElementById('audioLevelBar');\n        \n        // Analysis elements\n        this.faceAnalysisOverlay = document.getElementById('faceAnalysisOverlay');\n        this.confidenceScore = document.getElementById('confidenceScore');\n        this.eyeContactStatus = document.getElementById('eyeContactStatus');\n        this.expressionStatus = document.getElementById('expressionStatus');\n        \n        // Live analysis bars\n        this.liveConfidence = document.getElementById('liveConfidence');\n        this.speechClarity = document.getElementById('speechClarity');\n        this.engagementLevel = document.getElementById('engagementLevel');\n        \n        // State management\n        this.isRecording = false;\n        this.startTime = null;\n        this.timerInterval = null;\n        this.audioContext = null;\n        this.analyser = null;\n        this.faceAnalysisInterval = null;\n        this.speechRecognition = null;\n        this.transcriptText = '';\n        \n        // Face detection\n        this.faceDetector = null;\n        this.lastFaceAnalysis = null;\n        \n        this.bindEvents();\n    }\n    \n    async initialize() {\n        try {\n            await this.setupMediaDevices();\n            await this.setupFaceDetection();\n            await this.setupSpeechRecognition();\n            this.setupAudioAnalysis();\n            this.updateUI();\n            \n            console.log('Interview session initialized successfully');\n        } catch (error) {\n            console.error('Failed to initialize interview session:', error);\n            this.showError('Failed to initialize camera and microphone. Please check your permissions.');\n        }\n    }\n    \n    async setupMediaDevices() {\n        try {\n            this.stream = await navigator.mediaDevices.getUserMedia({\n                video: {\n                    width: { ideal: 1280 },\n                    height: { ideal: 720 },\n                    facingMode: 'user'\n                },\n                audio: {\n                    echoCancellation: true,\n                    noiseSuppression: true,\n                    autoGainControl: true\n                }\n            });\n            \n            this.videoElement.srcObject = this.stream;\n            \n            // Setup media recorder\n            this.mediaRecorder = new MediaRecorder(this.stream, {\n                mimeType: 'video/webm;codecs=vp8,opus'\n            });\n            \n            this.mediaRecorder.ondataavailable = (event) => {\n                if (event.data.size > 0) {\n                    this.videoChunks.push(event.data);\n                }\n            };\n            \n            this.mediaRecorder.onstop = () => {\n                this.processRecording();\n            };\n            \n        } catch (error) {\n            throw new Error('Camera and microphone access denied or not available');\n        }\n    }\n    \n    async setupFaceDetection() {\n        // Initialize face detection if available\n        if ('FaceDetector' in window) {\n            try {\n                this.faceDetector = new FaceDetector({\n                    fastMode: false,\n                    maxDetectedFaces: 1\n                });\n            } catch (error) {\n                console.warn('Face detection not supported:', error);\n            }\n        }\n        \n        // Fallback to manual face analysis\n        this.startFaceAnalysisSimulation();\n    }\n    \n    async setupSpeechRecognition() {\n        // Setup Web Speech API if available\n        if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {\n            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n            this.speechRecognition = new SpeechRecognition();\n            \n            this.speechRecognition.continuous = true;\n            this.speechRecognition.interimResults = true;\n            this.speechRecognition.lang = 'en-US';\n            \n            this.speechRecognition.onresult = (event) => {\n                let transcript = '';\n                for (let i = event.resultIndex; i < event.results.length; i++) {\n                    if (event.results[i].isFinal) {\n                        transcript += event.results[i][0].transcript + ' ';\n                    }\n                }\n                \n                if (transcript.trim()) {\n                    this.transcriptText += transcript;\n                    this.updateSpeechClarity(event.results[event.results.length - 1][0].confidence);\n                }\n            };\n            \n            this.speechRecognition.onerror = (event) => {\n                console.warn('Speech recognition error:', event.error);\n            };\n        }\n    }\n    \n    setupAudioAnalysis() {\n        try {\n            this.audioContext = new (window.AudioContext || window.webkitAudioContext)();\n            const source = this.audioContext.createMediaStreamSource(this.stream);\n            this.analyser = this.audioContext.createAnalyser();\n            \n            source.connect(this.analyser);\n            this.analyser.fftSize = 256;\n            \n            this.startAudioLevelMonitoring();\n        } catch (error) {\n            console.warn('Audio analysis setup failed:', error);\n        }\n    }\n    \n    startAudioLevelMonitoring() {\n        const bufferLength = this.analyser.frequencyBinCount;\n        const dataArray = new Uint8Array(bufferLength);\n        \n        const updateAudioLevel = () => {\n            if (!this.analyser) return;\n            \n            this.analyser.getByteFrequencyData(dataArray);\n            \n            // Calculate average volume\n            let sum = 0;\n            for (let i = 0; i < bufferLength; i++) {\n                sum += dataArray[i];\n            }\n            const average = sum / bufferLength;\n            const percentage = (average / 255) * 100;\n            \n            this.audioLevelBar.style.width = percentage + '%';\n            \n            // Update audio level color based on volume\n            if (percentage > 70) {\n                this.audioLevelBar.className = 'progress-bar bg-danger';\n            } else if (percentage > 40) {\n                this.audioLevelBar.className = 'progress-bar bg-success';\n            } else {\n                this.audioLevelBar.className = 'progress-bar bg-warning';\n            }\n            \n            requestAnimationFrame(updateAudioLevel);\n        };\n        \n        updateAudioLevel();\n    }\n    \n    startFaceAnalysisSimulation() {\n        // Simulate face analysis with random but realistic variations\n        this.faceAnalysisInterval = setInterval(() => {\n            if (!this.isRecording) return;\n            \n            // Simulate face analysis results\n            const confidence = 0.6 + (Math.random() * 0.3); // 60-90%\n            const eyeContact = Math.random() > 0.3; // 70% chance of eye contact\n            const happiness = 0.4 + (Math.random() * 0.4); // 40-80%\n            const nervousness = 0.2 + (Math.random() * 0.3); // 20-50%\n            const concentration = 0.5 + (Math.random() * 0.4); // 50-90%\n            \n            this.lastFaceAnalysis = {\n                confidence,\n                happiness,\n                nervousness,\n                concentration,\n                looking_at_camera: eyeContact,\n                head_position_x: (Math.random() - 0.5) * 0.2,\n                head_position_y: (Math.random() - 0.5) * 0.2\n            };\n            \n            this.updateFaceAnalysisUI();\n            this.saveFaceAnalysisData();\n            \n        }, 2000); // Update every 2 seconds\n    }\n    \n    updateFaceAnalysisUI() {\n        if (!this.lastFaceAnalysis) return;\n        \n        const analysis = this.lastFaceAnalysis;\n        \n        // Update overlay\n        this.confidenceScore.textContent = Math.round(analysis.confidence * 100);\n        this.eyeContactStatus.textContent = analysis.looking_at_camera ? 'Good' : 'Poor';\n        this.expressionStatus.textContent = analysis.happiness > 0.6 ? 'Positive' : 'Neutral';\n        \n        // Update live analysis bars\n        this.liveConfidence.style.width = (analysis.confidence * 100) + '%';\n        this.engagementLevel.style.width = (analysis.concentration * 100) + '%';\n        \n        // Show/hide overlay\n        this.faceAnalysisOverlay.style.display = this.isRecording ? 'block' : 'none';\n    }\n    \n    updateSpeechClarity(confidence) {\n        if (confidence && this.speechClarity) {\n            this.speechClarity.style.width = (confidence * 100) + '%';\n        }\n    }\n    \n    async saveFaceAnalysisData() {\n        if (!this.lastFaceAnalysis || !this.isRecording) return;\n        \n        const timestamp = (Date.now() - this.startTime) / 1000; // seconds since recording start\n        \n        try {\n            const response = await fetch('/save_face_analysis', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                },\n                body: JSON.stringify({\n                    interview_id: this.interviewId,\n                    timestamp: timestamp,\n                    analysis: this.lastFaceAnalysis\n                })\n            });\n            \n            if (!response.ok) {\n                console.warn('Failed to save face analysis data');\n            }\n        } catch (error) {\n            console.error('Error saving face analysis:', error);\n        }\n    }\n    \n    bindEvents() {\n        this.startBtn.addEventListener('click', () => this.startRecording());\n        this.stopBtn.addEventListener('click', () => this.stopRecording());\n        this.submitBtn.addEventListener('click', () => this.submitAnswer());\n        \n        // Handle page unload\n        window.addEventListener('beforeunload', () => {\n            this.cleanup();\n        });\n    }\n    \n    async startRecording() {\n        try {\n            this.isRecording = true;\n            this.startTime = Date.now();\n            this.videoChunks = [];\n            this.transcriptText = '';\n            \n            // Start media recording\n            this.mediaRecorder.start(1000); // Record in 1-second chunks\n            \n            // Start speech recognition\n            if (this.speechRecognition) {\n                this.speechRecognition.start();\n            }\n            \n            // Start timer\n            this.startTimer();\n            \n            // Update UI\n            this.updateRecordingUI(true);\n            \n            console.log('Recording started');\n            \n        } catch (error) {\n            console.error('Failed to start recording:', error);\n            this.showError('Failed to start recording');\n            this.isRecording = false;\n        }\n    }\n    \n    stopRecording() {\n        if (!this.isRecording) return;\n        \n        this.isRecording = false;\n        \n        // Stop media recorder\n        if (this.mediaRecorder && this.mediaRecorder.state !== 'inactive') {\n            this.mediaRecorder.stop();\n        }\n        \n        // Stop speech recognition\n        if (this.speechRecognition) {\n            this.speechRecognition.stop();\n        }\n        \n        // Stop timer\n        this.stopTimer();\n        \n        // Update UI\n        this.updateRecordingUI(false);\n        \n        console.log('Recording stopped');\n    }\n    \n    startTimer() {\n        this.timerInterval = setInterval(() => {\n            const elapsed = Date.now() - this.startTime;\n            const minutes = Math.floor(elapsed / 60000);\n            const seconds = Math.floor((elapsed % 60000) / 1000);\n            this.timer.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;\n        }, 1000);\n    }\n    \n    stopTimer() {\n        if (this.timerInterval) {\n            clearInterval(this.timerInterval);\n            this.timerInterval = null;\n        }\n    }\n    \n    processRecording() {\n        // Create blob from recorded chunks\n        const videoBlob = new Blob(this.videoChunks, { type: 'video/webm' });\n        \n        // For now, we'll just enable the submit button\n        // In a production environment, you might want to upload the video\n        this.submitBtn.disabled = false;\n        \n        console.log('Recording processed, ready to submit');\n    }\n    \n    async submitAnswer() {\n        const responseTime = this.startTime ? (Date.now() - this.startTime) / 1000 : 0;\n        \n        // Show processing modal\n        const processingModal = new bootstrap.Modal(document.getElementById('processingModal'));\n        processingModal.show();\n        \n        try {\n            const response = await fetch('/submit_answer', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                },\n                body: JSON.stringify({\n                    question_id: this.questionId,\n                    answer: this.transcriptText || 'Voice response recorded',\n                    response_time: responseTime,\n                    audio_file_path: '' // Would contain actual file path in production\n                })\n            });\n            \n            const result = await response.json();\n            \n            if (result.success) {\n                // Show brief feedback\n                this.showFeedback(result.analysis);\n                \n                // Redirect to next question or completion\n                setTimeout(() => {\n                    if (result.questions_remaining > 0) {\n                        window.location.reload(); // Load next question\n                    } else {\n                        window.location.href = `/complete_interview/${this.interviewId}`;\n                    }\n                }, 3000);\n            } else {\n                throw new Error(result.error || 'Failed to submit answer');\n            }\n            \n        } catch (error) {\n            console.error('Error submitting answer:', error);\n            this.showError('Failed to submit answer. Please try again.');\n        } finally {\n            processingModal.hide();\n        }\n    }\n    \n    showFeedback(analysis) {\n        // Create and show feedback toast\n        const toastContainer = document.createElement('div');\n        toastContainer.className = 'toast-container position-fixed top-0 end-0 p-3';\n        toastContainer.innerHTML = `\n            <div class=\"toast show\" role=\"alert\">\n                <div class=\"toast-header bg-success text-white\">\n                    <i class=\"fas fa-check-circle me-2\"></i>\n                    <strong class=\"me-auto\">Answer Submitted</strong>\n                </div>\n                <div class=\"toast-body\">\n                    <div class=\"mb-2\">\n                        <small>Quick Feedback:</small><br>\n                        ${analysis.feedback.substring(0, 100)}...\n                    </div>\n                    <div class=\"progress mb-2\" style=\"height: 6px;\">\n                        <div class=\"progress-bar bg-primary\" \n                             style=\"width: ${analysis.technical_accuracy * 100}%\"></div>\n                    </div>\n                    <small class=\"text-muted\">Technical Accuracy: ${Math.round(analysis.technical_accuracy * 100)}%</small>\n                </div>\n            </div>\n        `;\n        \n        document.body.appendChild(toastContainer);\n        \n        // Remove toast after 5 seconds\n        setTimeout(() => {\n            toastContainer.remove();\n        }, 5000);\n    }\n    \n    showError(message) {\n        // Create error alert\n        const alertContainer = document.querySelector('.container');\n        const errorAlert = document.createElement('div');\n        errorAlert.className = 'alert alert-danger alert-dismissible fade show';\n        errorAlert.innerHTML = `\n            <i class=\"fas fa-exclamation-triangle me-2\"></i>\n            ${message}\n            <button type=\"button\" class=\"btn-close\" data-bs-dismiss=\"alert\"></button>\n        `;\n        \n        alertContainer.insertBefore(errorAlert, alertContainer.firstChild);\n        \n        // Auto-dismiss after 5 seconds\n        setTimeout(() => {\n            if (errorAlert.parentNode) {\n                errorAlert.remove();\n            }\n        }, 5000);\n    }\n    \n    updateRecordingUI(recording) {\n        this.startBtn.disabled = recording;\n        this.stopBtn.disabled = !recording;\n        \n        if (recording) {\n            this.recordingStatus.textContent = 'Recording';\n            this.recordingStatus.className = 'badge bg-danger';\n            this.recordingIndicator.style.display = 'block';\n        } else {\n            this.recordingStatus.textContent = 'Stopped';\n            this.recordingStatus.className = 'badge bg-secondary';\n            this.recordingIndicator.style.display = 'none';\n            this.submitBtn.disabled = false;\n        }\n    }\n    \n    updateUI() {\n        // Initial UI state\n        this.startBtn.disabled = false;\n        this.stopBtn.disabled = true;\n        this.submitBtn.disabled = true;\n        this.recordingStatus.textContent = 'Ready';\n        this.recordingStatus.className = 'badge bg-success';\n    }\n    \n    cleanup() {\n        // Stop all intervals\n        if (this.timerInterval) {\n            clearInterval(this.timerInterval);\n        }\n        \n        if (this.faceAnalysisInterval) {\n            clearInterval(this.faceAnalysisInterval);\n        }\n        \n        // Stop recording if active\n        if (this.isRecording) {\n            this.stopRecording();\n        }\n        \n        // Stop speech recognition\n        if (this.speechRecognition) {\n            this.speechRecognition.stop();\n        }\n        \n        // Close audio context\n        if (this.audioContext) {\n            this.audioContext.close();\n        }\n        \n        // Stop media stream\n        if (this.stream) {\n            this.stream.getTracks().forEach(track => track.stop());\n        }\n    }\n}\n\n// Make InterviewSession available globally\nwindow.InterviewSession = InterviewSession;\n","size_bytes":19054}}}